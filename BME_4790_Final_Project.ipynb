{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniog18/BME-4790-Projects/blob/main/BME_4790_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"awsaf49/brats2020-training-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65MbiXMihfMh",
        "outputId": "311c4163-73fe-41da-e8c8-ad08995a32e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'brats2020-training-data' dataset.\n",
            "Path to dataset files: /kaggle/input/brats2020-training-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5292b971"
      },
      "source": [
        "# Task\n",
        "Train a UNET model for brain tumor segmentation using the BraTS 2020 dataset located at \"/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3\", and then summarize the model's performance based on the evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e4ebfe1"
      },
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries such as `nibabel` for NIfTI file handling and `monai` for medical image AI, along with standard libraries like `numpy` and `torch`. Then, import all required modules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecb9bcda"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install the necessary libraries, `nibabel` and `monai`, using pip as specified in the instructions. This needs to be in a separate code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f35ab7f",
        "outputId": "f55272cc-2700-4f13-c9ba-eb604b1b1c34"
      },
      "source": [
        "!pip install nibabel monai\n",
        "print(\"nibabel and monai installed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "nibabel and monai installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88a45225"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries are installed, I will import all required modules as specified in the instructions to prepare the environment for data loading, preprocessing, model definition, and training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abef8092",
        "outputId": "16f8880c-dcd1-403a-f3fe-04cd23eab7e7"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import Compose, LoadImaged, Orientationd, ScaleIntensityRanged, CropForegroundd, AsDiscrete, EnsureChannelFirstd\n",
        "from monai.data import decollate_batch, Dataset, DataLoader\n",
        "from monai.networks.nets import UNet\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "print(\"Required libraries imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bb9deb9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `AddChanneld` could not be imported from `monai.transforms`. This error usually indicates that the transform has been moved or renamed in the installed MONAI version (1.5.1). I will replace it with `EnsureChannelFirstd`, which serves a similar purpose for dictionary-based transforms and is a more common and robust way to handle channel dimensions in recent MONAI versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e836092"
      },
      "source": [
        "## Prepare Data Paths\n",
        "\n",
        "### Subtask:\n",
        "Utilize the `path` variable from the Kaggle dataset download to identify and organize the paths to the individual NIfTI image files (T1, T1Gd, T2, T2-FLAIR) and their corresponding segmentation masks for each patient in the BraTS dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efbf9d18"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the base path, import the `os` module, iterate through the dataset directory to find patient folders, and then construct the file paths for the different MRI modalities (T1, T1Gd, T2, T2-FLAIR) and their corresponding segmentation masks for each patient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebfee65",
        "outputId": "1a57a14d-9960-4d46-a2c1-c4f71b918530"
      },
      "source": [
        "import os\n",
        "\n",
        "base_path = '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/'\n",
        "\n",
        "data_dicts = []\n",
        "\n",
        "# List all patient directories\n",
        "patient_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
        "\n",
        "for patient_id in patient_dirs:\n",
        "    patient_path = os.path.join(base_path, patient_id)\n",
        "\n",
        "    # Construct paths for image modalities and segmentation mask\n",
        "    # Assuming the naming convention: <patient_id>_<modality>.nii.gz\n",
        "    image_t1 = os.path.join(patient_path, f\"{patient_id}_t1.nii.gz\")\n",
        "    image_t1gd = os.path.join(patient_path, f\"{patient_id}_t1ce.nii.gz\") # Often t1ce for contrast-enhanced T1\n",
        "    image_t2 = os.path.join(patient_path, f\"{patient_id}_t2.nii.gz\")\n",
        "    image_flair = os.path.join(patient_path, f\"{patient_id}_flair.nii.gz\")\n",
        "    label_seg = os.path.join(patient_path, f\"{patient_id}_seg.nii.gz\")\n",
        "\n",
        "    data_dicts.append({\n",
        "        \"image\": [image_t1, image_t1gd, image_t2, image_flair],\n",
        "        \"label\": label_seg\n",
        "    })\n",
        "\n",
        "print(f\"Found {len(data_dicts)} patient data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "split_idx = int(len(data_dicts) * 0.8)\n",
        "train_files = data_dicts[:split_idx]\n",
        "val_files = data_dicts[split_idx:]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files:\n",
        "    print(\"First entry of train_files:\", train_files[0])\n",
        "else:\n",
        "    print(\"train_files is empty.\")\n",
        "if val_files:\n",
        "    print(\"First entry of val_files:\", val_files[0])\n",
        "else:\n",
        "    print(\"val_files is empty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 patient data entries.\n",
            "Training files: 0\n",
            "Validation files: 1\n",
            "train_files is empty.\n",
            "First entry of val_files: {'image': ['/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/content_t1.nii.gz', '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/content_t1ce.nii.gz', '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/content_t2.nii.gz', '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/content_flair.nii.gz'], 'label': '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/content_seg.nii.gz'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f78172f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code output indicated that only one patient directory ('content') was found, leading to an empty `train_files` list. This suggests that the `base_path` was still one level too high in the directory structure. The actual patient folders are likely located inside the `content` directory within `BraTS2020_training_data`. I need to correct the `base_path` to include `content/` so that `os.listdir` can correctly find all individual patient directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32c3c597",
        "outputId": "21efbf61-7031-40a6-98a0-7df923a1ce37"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "base_path = '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/'\n",
        "\n",
        "data_dicts = []\n",
        "\n",
        "# Get all NIfTI files in the base_path\n",
        "all_nifti_files = glob.glob(os.path.join(base_path, \"*.nii.gz\"))\n",
        "\n",
        "# Extract unique patient IDs from the filenames\n",
        "patient_ids = set()\n",
        "for filepath in all_nifti_files:\n",
        "    filename = os.path.basename(filepath)\n",
        "    # Assuming filename format is BraTS20_XXXXX_modality.nii.gz or BraTS20_XXXXX_seg.nii.gz\n",
        "    # The patient ID is the part before the last underscore\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) >= 2:\n",
        "        # patient_id will be something like 'BraTS20_00001'\n",
        "        patient_id = '_'.join(parts[:-1])\n",
        "        if patient_id.startswith('BraTS20'): # Filter to ensure it's a valid patient ID\n",
        "            patient_ids.add(patient_id)\n",
        "\n",
        "# Sort patient IDs for consistent order\n",
        "sorted_patient_ids = sorted(list(patient_ids))\n",
        "\n",
        "# For each unique patient ID, construct the data dictionary\n",
        "for patient_id in sorted_patient_ids:\n",
        "    image_t1 = os.path.join(base_path, f\"{patient_id}_t1.nii.gz\")\n",
        "    image_t1gd = os.path.join(base_path, f\"{patient_id}_t1ce.nii.gz\") # Often t1ce for contrast-enhanced T1\n",
        "    image_t2 = os.path.join(base_path, f\"{patient_id}_t2.nii.gz\")\n",
        "    image_flair = os.path.join(base_path, f\"{patient_id}_flair.nii.gz\")\n",
        "    label_seg = os.path.join(base_path, f\"{patient_id}_seg.nii.gz\")\n",
        "\n",
        "    # Ensure all expected files exist for this patient\n",
        "    if (os.path.exists(image_t1) and os.path.exists(image_t1gd) and\n",
        "        os.path.exists(image_t2) and os.path.exists(image_flair) and\n",
        "        os.path.exists(label_seg)):\n",
        "\n",
        "        data_dicts.append({\n",
        "            \"image\": [image_t1, image_t1gd, image_t2, image_flair],\n",
        "            \"label\": label_seg\n",
        "        })\n",
        "\n",
        "print(f\"Found {len(data_dicts)} patient data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "split_idx = int(len(data_dicts) * 0.8)\n",
        "train_files = data_dicts[:split_idx]\n",
        "val_files = data_dicts[split_idx:]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files:\n",
        "    print(\"First entry of train_files:\", train_files[0])\n",
        "else:\n",
        "    print(\"train_files is empty.\")\n",
        "if val_files:\n",
        "    print(\"First entry of val_files:\", val_files[0])\n",
        "else:\n",
        "    print(\"val_files is empty.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 patient data entries.\n",
            "Training files: 0\n",
            "Validation files: 0\n",
            "train_files is empty.\n",
            "val_files is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e981711"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts failed to correctly identify all patient files, likely due to unexpected nesting or a non-standard directory structure for the BraTS dataset from this Kaggle source. To robustly find all NIfTI files and their corresponding patient IDs, I will perform a recursive search for all `.nii.gz` files starting from the top-level dataset path. Then, I will extract unique patient IDs from these filenames and group the modality images and segmentation masks for each patient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbe76a4",
        "outputId": "e2109e5a-fb38-4b63-b41c-456d383bdf03"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Set the base path to the top-level directory where the dataset was extracted\n",
        "base_dataset_path = '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/'\n",
        "\n",
        "data_dicts = []\n",
        "\n",
        "# Find all NIfTI files recursively within the base_dataset_path\n",
        "all_nifti_files = glob.glob(os.path.join(base_dataset_path, '**', '*.nii.gz'), recursive=True)\n",
        "\n",
        "# Dictionary to store files for each patient ID\n",
        "patient_files = {}\n",
        "\n",
        "for filepath in all_nifti_files:\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    # Extract patient_id from filename (e.g., BraTS20_00001_t1.nii.gz -> BraTS20_00001)\n",
        "    # Or for the observed 'data_t1.nii.gz' -> 'data'\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) >= 2: # Ensure there's a modality/seg part\n",
        "        patient_id = '_'.join(parts[:-1])\n",
        "\n",
        "        # Get the directory where this patient's files are located\n",
        "        patient_dir = os.path.dirname(filepath)\n",
        "\n",
        "        if patient_id not in patient_files:\n",
        "            patient_files[patient_id] = {'dir': patient_dir, 'files': []}\n",
        "        patient_files[patient_id]['files'].append(filepath)\n",
        "\n",
        "# Now, for each patient_id, construct the full data dictionary\n",
        "for patient_id, info in patient_files.items():\n",
        "    patient_dir = info['dir']\n",
        "\n",
        "    image_t1 = os.path.join(patient_dir, f\"{patient_id}_t1.nii.gz\")\n",
        "    image_t1gd = os.path.join(patient_dir, f\"{patient_id}_t1ce.nii.gz\")\n",
        "    image_t2 = os.path.join(patient_dir, f\"{patient_id}_t2.nii.gz\")\n",
        "    image_flair = os.path.join(patient_dir, f\"{patient_id}_flair.nii.gz\")\n",
        "    label_seg = os.path.join(patient_dir, f\"{patient_id}_seg.nii.gz\")\n",
        "\n",
        "    # Ensure all expected files exist for this patient\n",
        "    # Note: This might filter out patients with incomplete data\n",
        "    if (os.path.exists(image_t1) and os.path.exists(image_t1gd) and\n",
        "        os.path.exists(image_t2) and os.path.exists(image_flair) and\n",
        "        os.path.exists(label_seg)):\n",
        "\n",
        "        data_dicts.append({\n",
        "            \"image\": [image_t1, image_t1gd, image_t2, image_flair],\n",
        "            \"label\": label_seg\n",
        "        })\n",
        "\n",
        "print(f\"Found {len(data_dicts)} patient data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "split_idx = int(len(data_dicts) * 0.8)\n",
        "train_files = data_dicts[:split_idx]\n",
        "val_files = data_dicts[split_idx:]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files:\n",
        "    print(\"First entry of train_files:\", train_files[0])\n",
        "else:\n",
        "    print(\"train_files is empty.\")\n",
        "if val_files:\n",
        "    print(\"First entry of val_files:\", val_files[0])\n",
        "else:\n",
        "    print(\"val_files is empty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 patient data entries.\n",
            "Training files: 0\n",
            "Validation files: 0\n",
            "train_files is empty.\n",
            "val_files is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66b56d89"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block resulted in an empty `all_nifti_files` list, meaning no `.nii.gz` files were found recursively within the `base_dataset_path`. This suggests an issue with the path or the actual presence of the files. To diagnose this, I will first list the immediate contents of the `base_dataset_path` and then retry the recursive `glob.glob` with a print statement to confirm how many files are found. If `glob` still yields no files, I will use `os.walk` as a more robust way to enumerate all `.nii.gz` files and explicitly print their paths to understand the actual directory structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3dd286",
        "outputId": "1d44d91e-4b88-4d16-b5c6-9181027ad94c"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Set the base path to the top-level directory where the dataset was extracted\n",
        "base_dataset_path = '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/'\n",
        "\n",
        "print(f\"Listing contents of: {base_dataset_path}\")\n",
        "# List direct contents of the base path to inspect structure\n",
        "try:\n",
        "    print(os.listdir(base_dataset_path))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {base_dataset_path} not found.\")\n",
        "\n",
        "data_dicts = []\n",
        "\n",
        "# Find all NIfTI files recursively within the base_dataset_path\n",
        "# This is a re-attempt of the previous glob with a diagnostic print\n",
        "all_nifti_files = glob.glob(os.path.join(base_dataset_path, '**', '*.nii.gz'), recursive=True)\n",
        "print(f\"Found {len(all_nifti_files)} NIfTI files using recursive glob.\")\n",
        "\n",
        "# If glob still finds nothing, use os.walk for explicit enumeration and inspection\n",
        "if not all_nifti_files:\n",
        "    print(\"Recursive glob found no files. Attempting os.walk to find .nii.gz files.\")\n",
        "    for root, _, files in os.walk(base_dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.nii.gz'):\n",
        "                all_nifti_files.append(os.path.join(root, file))\n",
        "    print(f\"Found {len(all_nifti_files)} NIfTI files using os.walk.\")\n",
        "    if all_nifti_files:\n",
        "        print(\"First 5 NIfTI files found (if any):\\n\", '\\n'.join(all_nifti_files[:5]))\n",
        "    else:\n",
        "        print(\"No NIfTI files found even with os.walk. Dataset might be missing or path is wrong.\")\n",
        "\n",
        "# Dictionary to store files for each patient ID\n",
        "patient_files = {}\n",
        "\n",
        "for filepath in all_nifti_files:\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    # Attempt to extract patient_id. BraTS filenames are typically BraTS20_XXXXX_modality.nii.gz\n",
        "    # Or, as observed previously, 'data_modality.nii.gz' if a simplified dataset was provided.\n",
        "    parts = filename.split('_')\n",
        "    patient_id_found = False\n",
        "    if len(parts) >= 2: # At least 'ID_modality.nii.gz'\n",
        "        if parts[0].startswith('BraTS20'): # Typical BraTS ID\n",
        "            patient_id = '_'.join(parts[:-1])\n",
        "            patient_id_found = True\n",
        "        elif parts[0] == 'data' and len(parts) == 2: # e.g., 'data_t1.nii.gz'\n",
        "            patient_id = parts[0]\n",
        "            patient_id_found = True\n",
        "\n",
        "    if patient_id_found:\n",
        "        patient_dir = os.path.dirname(filepath)\n",
        "\n",
        "        if patient_id not in patient_files:\n",
        "            patient_files[patient_id] = {'dir': patient_dir, 'files': []}\n",
        "        patient_files[patient_id]['files'].append(filepath)\n",
        "    else:\n",
        "        print(f\"Warning: Could not extract patient ID from {filename}\")\n",
        "\n",
        "# Now, for each unique patient_id, construct the full data dictionary\n",
        "for patient_id, info in patient_files.items():\n",
        "    patient_dir = info['dir']\n",
        "\n",
        "    image_t1 = os.path.join(patient_dir, f\"{patient_id}_t1.nii.gz\")\n",
        "    image_t1gd = os.path.join(patient_dir, f\"{patient_id}_t1ce.nii.gz\")\n",
        "    image_t2 = os.path.join(patient_dir, f\"{patient_id}_t2.nii.gz\")\n",
        "    image_flair = os.path.join(patient_dir, f\"{patient_id}_flair.nii.gz\")\n",
        "    label_seg = os.path.join(patient_dir, f\"{patient_id}_seg.nii.gz\")\n",
        "\n",
        "    # Ensure all expected files exist for this patient\n",
        "    # This filters out patients with incomplete data or incorrectly inferred IDs\n",
        "    if (os.path.exists(image_t1) and os.path.exists(image_t1gd) and\n",
        "        os.path.exists(image_t2) and os.path.exists(image_flair) and\n",
        "        os.path.exists(label_seg)): # All modalities and segmentation mask must exist\n",
        "\n",
        "        data_dicts.append({\n",
        "            \"image\": [image_t1, image_t1gd, image_t2, image_flair],\n",
        "            \"label\": label_seg\n",
        "        })\n",
        "\n",
        "print(f\"Final found {len(data_dicts)} patient data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "# Handle case where data_dicts might be empty to avoid division by zero or IndexError\n",
        "if len(data_dicts) > 0:\n",
        "    split_idx = int(len(data_dicts) * 0.8)\n",
        "    train_files = data_dicts[:split_idx]\n",
        "    val_files = data_dicts[split_idx:]\n",
        "else:\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files:\n",
        "    print(\"First entry of train_files:\", train_files[0])\n",
        "else:\n",
        "    print(\"train_files is empty.\")\n",
        "if val_files:\n",
        "    print(\"First entry of val_files:\", val_files[0])\n",
        "else:\n",
        "    print(\"val_files is empty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of: /root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/\n",
            "['BraTS20 Training Metadata.csv', 'BraTS2020_training_data']\n",
            "Found 0 NIfTI files using recursive glob.\n",
            "Recursive glob found no files. Attempting os.walk to find .nii.gz files.\n",
            "Found 0 NIfTI files using os.walk.\n",
            "No NIfTI files found even with os.walk. Dataset might be missing or path is wrong.\n",
            "Final found 0 patient data entries.\n",
            "Training files: 0\n",
            "Validation files: 0\n",
            "train_files is empty.\n",
            "val_files is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9e2828"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed to find any NIfTI files because the `base_dataset_path` was set one level too high. Based on the `os.listdir` output, the NIfTI files are likely located within the `BraTS2020_training_data` subdirectory. I need to correct the `base_dataset_path` to include this directory and then re-execute the file search and organization logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dc6dcd8",
        "outputId": "358d8390-8a13-450d-8969-61dd13a41f9f"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Corrected base path to the directory containing patient data\n",
        "base_dataset_path = '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/'\n",
        "\n",
        "data_dicts = []\n",
        "\n",
        "print(f\"Listing contents of: {base_dataset_path}\")\n",
        "try:\n",
        "    # Use os.walk to find all .nii.gz files, assuming they could be nested\n",
        "    all_nifti_files = []\n",
        "    for root, _, files in os.walk(base_dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.nii.gz'):\n",
        "                all_nifti_files.append(os.path.join(root, file))\n",
        "    print(f\"Found {len(all_nifti_files)} NIfTI files using os.walk.\")\n",
        "    if all_nifti_files:\n",
        "        print(\"First 5 NIfTI files found (if any):\\n\", '\\n'.join(all_nifti_files[:5]))\n",
        "    else:\n",
        "        print(\"No NIfTI files found even with os.walk. Dataset might be missing or path is wrong.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {base_dataset_path} not found. Please check the dataset path.\")\n",
        "    all_nifti_files = []\n",
        "\n",
        "# Dictionary to store files for each patient ID\n",
        "patient_files = {}\n",
        "\n",
        "for filepath in all_nifti_files:\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    # Extract patient_id from filename (e.g., BraTS20_00001_t1.nii.gz -> BraTS20_00001)\n",
        "    # The format can be 'BraTS20_XXXXX_modality.nii.gz' or 'data_modality.nii.gz'\n",
        "    parts = filename.split('_')\n",
        "    patient_id_found = False\n",
        "    patient_id = None\n",
        "\n",
        "    if len(parts) >= 2:\n",
        "        # Check for typical BraTS ID format\n",
        "        if parts[0].startswith('BraTS20'):\n",
        "            patient_id = '_'.join(parts[:-1])\n",
        "            patient_id_found = True\n",
        "        # Check for the simplified 'data_modality.nii.gz' format if it's the only patient\n",
        "        elif parts[0] == 'data' and len(parts) >= 2 and any(mod in filename for mod in ['t1', 't1ce', 't2', 'flair', 'seg']): # Ensure it's a modality file\n",
        "            patient_id = parts[0]\n",
        "            patient_id_found = True\n",
        "\n",
        "    if patient_id_found and patient_id:\n",
        "        patient_dir = os.path.dirname(filepath)\n",
        "\n",
        "        if patient_id not in patient_files:\n",
        "            patient_files[patient_id] = {'dir': patient_dir, 'files': []}\n",
        "        patient_files[patient_id]['files'].append(filepath)\n",
        "    else:\n",
        "        # This warning helps diagnose if patient IDs are not extracted correctly\n",
        "        print(f\"Warning: Could not reliably extract patient ID from filename: {filename}\")\n",
        "\n",
        "# Now, for each unique patient_id, construct the full data dictionary\n",
        "for patient_id, info in patient_files.items():\n",
        "    patient_dir = info['dir']\n",
        "\n",
        "    # Assuming consistent naming for modalities and segmentation mask\n",
        "    image_t1 = os.path.join(patient_dir, f\"{patient_id}_t1.nii.gz\")\n",
        "    image_t1gd = os.path.join(patient_dir, f\"{patient_id}_t1ce.nii.gz\")\n",
        "    image_t2 = os.path.join(patient_dir, f\"{patient_id}_t2.nii.gz\")\n",
        "    image_flair = os.path.join(patient_dir, f\"{patient_id}_flair.nii.gz\")\n",
        "    label_seg = os.path.join(patient_dir, f\"{patient_id}_seg.nii.gz\")\n",
        "\n",
        "    # Ensure all expected files exist for this patient before adding to data_dicts\n",
        "    if (os.path.exists(image_t1) and os.path.exists(image_t1gd) and\n",
        "        os.path.exists(image_t2) and os.path.exists(image_flair) and\n",
        "        os.path.exists(label_seg)):\n",
        "\n",
        "        data_dicts.append({\n",
        "            \"image\": [image_t1, image_t1gd, image_t2, image_flair],\n",
        "            \"label\": label_seg\n",
        "        })\n",
        "\n",
        "print(f\"Final found {len(data_dicts)} patient data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "if len(data_dicts) > 0:\n",
        "    split_idx = int(len(data_dicts) * 0.8)\n",
        "    train_files = data_dicts[:split_idx]\n",
        "    val_files = data_dicts[split_idx:]\n",
        "else:\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files:\n",
        "    print(\"First entry of train_files:\", train_files[0])\n",
        "else:\n",
        "    print(\"train_files is empty.\")\n",
        "if val_files:\n",
        "    print(\"First entry of val_files:\", val_files[0])\n",
        "else:\n",
        "    print(\"val_files is empty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of: /root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/\n",
            "Found 0 NIfTI files using os.walk.\n",
            "No NIfTI files found even with os.walk. Dataset might be missing or path is wrong.\n",
            "Final found 0 patient data entries.\n",
            "Training files: 0\n",
            "Validation files: 0\n",
            "train_files is empty.\n",
            "val_files is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532a5559"
      },
      "source": [
        "### Data Format Mismatch\n",
        "\n",
        "The current dataset at `/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/` does not appear to contain any NIfTI (`.nii.gz`) files, which are typically used for medical image segmentation tasks like BraTS. The `os.walk` command, which performs a thorough recursive search, found no such files.\n",
        "\n",
        "Upon reviewing the kernel state, it seems there are `.h5` (HDF5) files present (e.g., `volume_159_slice_141.h5`). If the dataset provided is indeed in `.h5` format, the subsequent steps of loading and processing NIfTI files will fail.\n",
        "\n",
        "To proceed, please clarify the expected file format of the dataset or confirm if a different dataset containing NIfTI files should be used. As the subtask explicitly requires processing NIfTI files, I am unable to continue without this clarification or a corrected dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dccaf7be"
      },
      "source": [
        "## Prepare Data Paths (Retry: Verify Dataset Format)\n",
        "\n",
        "### Subtask:\n",
        "Re-verify the contents of the dataset path to confirm the absence of NIfTI files, and check for the presence of alternative formats like HDF5 (.h5) files, as the previous attempts to locate NIfTI files failed. If no NIfTI files are found, indicate that the task cannot proceed with the current dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69cc158d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to explicitly list the contents of the base dataset path, then recursively search for `.nii.gz` and `.h5` files using `os.walk` to verify the dataset format, as previous attempts to locate NIfTI files failed. This will help determine if the data is present in an unexpected format or if the path is still incorrect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29cea24c"
      },
      "source": [
        "## Finish task - Data Format Mismatch\n",
        "\n",
        "### Subtask:\n",
        "Summarize the critical issue encountered and state why the task cannot proceed as planned.\n",
        "\n",
        "**Summary of Issue:**\n",
        "\n",
        "The core data preparation step has failed because the dataset located at `/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/` does not contain the expected NIfTI (`.nii.gz`) files, which are essential for this BraTS segmentation task. Instead, the recursive search revealed that the dataset primarily consists of HDF5 (`.h5`) files.\n",
        "\n",
        "Due to this fundamental data format mismatch, the current task plan, which assumes the availability and processing of NIfTI files for preprocessing, model training, and evaluation using MONAI's NIfTI-specific functionalities, cannot be executed.\n",
        "\n",
        "Therefore, the task cannot proceed further as designed until either:\n",
        "1. A dataset in the correct NIfTI format is provided.\n",
        "2. The task's requirements are adjusted to process the discovered HDF5 files, which would necessitate significant changes to the data loading and preprocessing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adff1f9"
      },
      "source": [
        "## Inspect HDF5 Data Structure\n",
        "\n",
        "### Subtask:\n",
        "Investigate the internal structure of the discovered HDF5 (.h5) files to understand how the multi-modal images (T1, T1Gd, T2, T2-FLAIR) and their corresponding segmentation masks are stored within each file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09a7923a"
      },
      "source": [
        "**Reasoning**:\n",
        "Since I have confirmed that the dataset contains HDF5 files, I will now inspect the internal structure of a sample HDF5 file using `h5py` to understand how the image modalities and segmentation masks are stored, as this is crucial for data loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb1f9a47",
        "outputId": "a9272f56-66b4-4155-cd22-299f818f2501"
      },
      "source": [
        "import h5py\n",
        "\n",
        "# Select a sample .h5 file from the h5_files list\n",
        "if h5_files:\n",
        "    sample_h5_file = h5_files[0]\n",
        "    print(f\"Inspecting sample HDF5 file: {sample_h5_file}\")\n",
        "\n",
        "    try:\n",
        "        with h5py.File(sample_h5_file, 'r') as f:\n",
        "            print(\"\\nKeys within the HDF5 file:\")\n",
        "            for key in f.keys():\n",
        "                print(f\" - {key}\")\n",
        "                if isinstance(f[key], h5py.Dataset):\n",
        "                    print(f\"   Shape: {f[key].shape}, Dtype: {f[key].dtype}\")\n",
        "                elif isinstance(f[key], h5py.Group):\n",
        "                    # If it's a group, print its immediate members\n",
        "                    print(\"   Group members:\")\n",
        "                    for sub_key in f[key].keys():\n",
        "                        print(f\"    - {sub_key}\")\n",
        "                        if isinstance(f[key][sub_key], h5py.Dataset):\n",
        "                            print(f\"      Shape: {f[key][sub_key].shape}, Dtype: {f[key][sub_key].dtype}\")\n",
        "\n",
        "            # Attempt to access typical data keys based on common HDF5 medical imaging structures\n",
        "            # and previous errors indicating 'image' and 'mask' were expected\n",
        "            print(\"\\nDetailed inspection of potential 'image' and 'mask' keys:\")\n",
        "            if 'image' in f:\n",
        "                print(f\"'image' dataset found. Shape: {f['image'].shape}, Dtype: {f['image'].dtype}\")\n",
        "            else:\n",
        "                print(\"No 'image' key found directly.\")\n",
        "\n",
        "            if 'mask' in f:\n",
        "                print(f\"'mask' dataset found. Shape: {f['mask'].shape}, Dtype: {f['mask'].dtype}\")\n",
        "            else:\n",
        "                print(\"No 'mask' key found directly.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening or inspecting HDF5 file {sample_h5_file}: {e}\")\n",
        "else:\n",
        "    print(\"No HDF5 files found in the 'h5_files' list. Cannot perform inspection.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting sample HDF5 file: /root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/volume_237_slice_51.h5\n",
            "\n",
            "Keys within the HDF5 file:\n",
            " - image\n",
            "   Shape: (240, 240, 4), Dtype: float64\n",
            " - mask\n",
            "   Shape: (240, 240, 3), Dtype: uint8\n",
            "\n",
            "Detailed inspection of potential 'image' and 'mask' keys:\n",
            "'image' dataset found. Shape: (240, 240, 4), Dtype: float64\n",
            "'mask' dataset found. Shape: (240, 240, 3), Dtype: uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa3dffe2"
      },
      "source": [
        "### Summary of HDF5 Data Structure\n",
        "\n",
        "The inspection of a sample HDF5 file (`volume_237_slice_51.h5`) revealed that each `.h5` file contains two primary datasets:\n",
        "\n",
        "1.  **`image`**: This dataset has a shape of `(240, 240, 4)` and a `dtype` of `float64`. This structure indicates that it stores a 2D image slice where the last dimension (size 4) represents the four different MRI modalities (T1, T1Gd, T2, T2-FLAIR) stacked together as channels for a single slice.\n",
        "2.  **`mask`**: This dataset has a shape of `(240, 240, 3)` and a `dtype` of `uint8`. This structure suggests that it stores the corresponding 2D segmentation mask for the image slice, with the last dimension (size 3) likely representing different tumor subregions (e.g., enhancing tumor, edema, necrotic core) as separate channels.\n",
        "\n",
        "Each HDF5 file therefore represents a single 2D slice from a 3D MRI volume, containing all four modalities and their associated 3-channel segmentation mask for that slice. This means the dataset is organized on a per-slice basis rather than per-3D-volume basis, and modalities are concatenated along the channel dimension within the 'image' dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7a8f1f"
      },
      "source": [
        "## Prepare Data Paths (for HDF5 files)\n",
        "\n",
        "### Subtask:\n",
        "Organize the discovered HDF5 (.h5) file paths and split them into training and validation sets, similar to how NIfTI file paths were intended to be prepared.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8291e6d5"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to organize the HDF5 file paths into dictionaries for training and validation, following the instructions to map both 'image' and 'label' to the HDF5 file path, and then split them into train and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da6c8490",
        "outputId": "8c07f829-1ebd-4b0f-98d4-7cd600ff10db"
      },
      "source": [
        "data_dicts_h5 = []\n",
        "\n",
        "# Iterate through the previously identified h5_files list\n",
        "for filepath in h5_files:\n",
        "    # Create a dictionary for each HDF5 file\n",
        "    # 'image' and 'label' both point to the same HDF5 file path\n",
        "    # as this file contains both image and mask data\n",
        "    data_dicts_h5.append({\n",
        "        \"image\": filepath,\n",
        "        \"label\": filepath\n",
        "    })\n",
        "\n",
        "print(f\"Found {len(data_dicts_h5)} HDF5 data entries.\")\n",
        "\n",
        "# Split data into training and validation sets (e.g., 80% train, 20% validation)\n",
        "# Handle case where data_dicts_h5 might be empty to avoid errors\n",
        "if len(data_dicts_h5) > 0:\n",
        "    split_idx = int(len(data_dicts_h5) * 0.8)\n",
        "    train_files_h5 = data_dicts_h5[:split_idx]\n",
        "    val_files_h5 = data_dicts_h5[split_idx:]\n",
        "else:\n",
        "    train_files_h5 = []\n",
        "    val_files_h5 = []\n",
        "\n",
        "print(f\"Training files (HDF5): {len(train_files_h5)}\")\n",
        "print(f\"Validation files (HDF5): {len(val_files_h5)}\")\n",
        "\n",
        "# Safely print first entries if lists are not empty\n",
        "if train_files_h5:\n",
        "    print(\"First entry of train_files_h5:\", train_files_h5[0])\n",
        "else:\n",
        "    print(\"train_files_h5 is empty.\")\n",
        "if val_files_h5:\n",
        "    print(\"First entry of val_files_h5:\", val_files_h5[0])\n",
        "else:\n",
        "    print(\"val_files_h5 is empty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57195 HDF5 data entries.\n",
            "Training files (HDF5): 45756\n",
            "Validation files (HDF5): 11439\n",
            "First entry of train_files_h5: {'image': '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/volume_237_slice_51.h5', 'label': '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/volume_237_slice_51.h5'}\n",
            "First entry of val_files_h5: {'image': '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/volume_315_slice_65.h5', 'label': '/root/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data/volume_315_slice_65.h5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a40f4b2"
      },
      "source": [
        "## Preprocess and Create Data Loaders\n",
        "\n",
        "### Subtask:\n",
        "Implement a custom PyTorch `Dataset` and `DataLoader` to handle the BraTS HDF5 files. This will involve reading the multi-modal MRI slices, performing preprocessing steps like intensity normalization, and ensuring channel-first format, and then splitting the data into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec0c77e2"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a custom `LoadHDF5d` transform to correctly load the image and mask data from the HDF5 files as identified in the previous step. This transform will open the HDF5 file, extract the 'image' and 'mask' datasets, and ensure they are converted to the correct data types (float32).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5945a7f",
        "outputId": "af13a579-f4dd-40fc-ff03-e075dce438af"
      },
      "source": [
        "import h5py\n",
        "from monai.transforms import MapTransform\n",
        "\n",
        "class LoadHDF5d(MapTransform):\n",
        "    \"\"\"Custom transform to load 'image' and 'mask' from an HDF5 file.\"\"\"\n",
        "\n",
        "    def __call__(self, data):\n",
        "        d = dict(data)\n",
        "        filepath = d[\"image\"] # Both 'image' and 'label' point to the same HDF5 file\n",
        "\n",
        "        with h5py.File(filepath, 'r') as f:\n",
        "            # Load image data and convert to float32\n",
        "            image_data = f['image'][()] # [()] reads the entire dataset into memory\n",
        "            d[\"image\"] = image_data.astype(np.float32)\n",
        "\n",
        "            # Load mask data and convert to float32\n",
        "            mask_data = f['mask'][()]\n",
        "            d[\"label\"] = mask_data.astype(np.float32) # Use float32 for masks as well for consistency with DiceLoss\n",
        "\n",
        "        return d\n",
        "\n",
        "print(\"Custom LoadHDF5d transform defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom LoadHDF5d transform defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6975976a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the custom HDF5 loader is defined, I will define the MONAI transforms for training and validation, and then create the Dataset and DataLoader instances for both, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cf1807",
        "outputId": "fd6c394a-c9da-40b5-b9ec-68d41c186bc8"
      },
      "source": [
        "from monai.transforms import Compose, EnsureChannelFirstd, ScaleIntensityRanged, Orientationd, CropForegroundd\n",
        "from monai.data import Dataset, DataLoader\n",
        "\n",
        "# 2. Define train_transforms\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadHDF5d(keys=[\"image\", \"label\"]), # Custom transform to load HDF5 data\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=2), # Change (H, W, C) to (C, H, W)\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-500.0,\n",
        "            a_max=3000.0,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        # Add other transforms as needed for training (e.g., augmentation, spatial transforms)\n",
        "        # For BraTS, typical transforms might include SpatialPadd, RandCropByPosNegLabeld, RandFlipd, RandRotated, etc.\n",
        "        # For now, keeping it simple as per the general instruction for preprocessing.\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Define val_transforms\n",
        "# For simplicity, using the same transforms as train_transforms, typically excluding augmentations\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadHDF5d(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-500.0,\n",
        "            a_max=3000.0,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Create train_ds and val_ds\n",
        "train_ds = Dataset(data=train_files_h5, transform=train_transforms)\n",
        "val_ds = Dataset(data=val_files_h5, transform=val_transforms)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "\n",
        "# 5. Create train_loader and val_loader\n",
        "batch_size = 8 # Can be adjusted based on GPU memory\n",
        "num_workers = 4 # Can be adjusted based on CPU cores\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f\"Training DataLoader created with batch size {batch_size} and {num_workers} workers.\")\n",
        "print(f\"Validation DataLoader created with batch size {batch_size} and {num_workers} workers.\")\n",
        "\n",
        "# Optionally, test fetching a batch\n",
        "for batch_data in train_loader:\n",
        "    print(f\"Sample train batch image shape: {batch_data['image'].shape}\")\n",
        "    print(f\"Sample train batch label shape: {batch_data['label'].shape}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 45756\n",
            "Validation dataset size: 11439\n",
            "Training DataLoader created with batch size 8 and 4 workers.\n",
            "Validation DataLoader created with batch size 8 and 4 workers.\n",
            "Sample train batch image shape: torch.Size([8, 4, 240, 240])\n",
            "Sample train batch label shape: torch.Size([8, 3, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e421678"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `EnsureChannelFirstd` could not determine the channel dimension of the HDF5 data, which is loaded as `(H, W, C)`. I need to explicitly specify `channel_dim=-1` in `EnsureChannelFirstd` to correctly transform the data to `(C, H, W)` format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e21ac1d7",
        "outputId": "942d3bfd-1e4f-4a71-d187-34031a3abac3"
      },
      "source": [
        "from monai.transforms import Compose, EnsureChannelFirstd, ScaleIntensityRanged, Orientationd, CropForegroundd\n",
        "from monai.data import Dataset, DataLoader\n",
        "\n",
        "# 2. Define train_transforms\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadHDF5d(keys=[\"image\", \"label\"]), # Custom transform to load HDF5 data\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=2), # Change (H, W, C) to (C, H, W), explicitly stating channel_dim\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-500.0,\n",
        "            a_max=3000.0,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        # Add other transforms as needed for training (e.g., augmentation, spatial transforms)\n",
        "        # For BraTS, typical transforms might include SpatialPadd, RandCropByPosNegLabeld, RandFlipd, RandRotated, etc.\n",
        "        # For now, keeping it simple as per the general instruction for preprocessing.\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Define val_transforms\n",
        "# For simplicity, using the same transforms as train_transforms, typically excluding augmentations\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadHDF5d(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=-1),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-500.0,\n",
        "            a_max=3000.0,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Create train_ds and val_ds\n",
        "train_ds = Dataset(data=train_files_h5, transform=train_transforms)\n",
        "val_ds = Dataset(data=val_files_h5, transform=val_transforms)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "\n",
        "# 5. Create train_loader and val_loader\n",
        "batch_size = 8 # Can be adjusted based on GPU memory\n",
        "num_workers = 4 # Can be adjusted based on CPU cores\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f\"Training DataLoader created with batch size {batch_size} and {num_workers} workers.\")\n",
        "print(f\"Validation DataLoader created with batch size {batch_size} and {num_workers} workers.\")\n",
        "\n",
        "# Optionally, test fetching a batch\n",
        "for batch_data in train_loader:\n",
        "    print(f\"Sample train batch image shape: {batch_data['image'].shape}\")\n",
        "    print(f\"Sample train batch label shape: {batch_data['label'].shape}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 45756\n",
            "Validation dataset size: 11439\n",
            "Training DataLoader created with batch size 8 and 4 workers.\n",
            "Validation DataLoader created with batch size 8 and 4 workers.\n",
            "Sample train batch image shape: torch.Size([8, 4, 240, 240])\n",
            "Sample train batch label shape: torch.Size([8, 3, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ae35f4b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully executed, creating the data loaders and verifying the shapes of the loaded batches. The warnings about `num_workers` are advisory and do not require a code fix. Now, I will proceed to the next logical step in the task, which is to define the UNET model as required for brain tumor segmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9a89039d",
        "outputId": "0e784ea1-6c8f-4ed1-cc37-f160251aefc9"
      },
      "source": [
        "## Define UNET Model\n",
        "\n",
        "### Subtask:\n",
        "Instantiate a UNET model from MONAI's 'monai.networks.nets'\n",
        "#module with appropriate input channels (4 for multi-modal MRI), output channels (3 for the segmentation mask), and a suitable number of feature maps and layers."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 4) (ipython-input-3053072020.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3053072020.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Instantiate a UNET model from MONAI's 'monai.networks.nets'\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bdf3c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to create a markdown cell failed due to incorrect cell type usage. The current task is to instantiate the UNET model in a Python code block. I will define the UNet model using the specified input and output channels based on the HDF5 data structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f04db2e"
      },
      "source": [
        "from monai.networks.nets import UNet\n",
        "\n",
        "# Define the UNET model\n",
        "# Input channels: 4 (T1, T1Gd, T2, FLAIR)\n",
        "# Output channels: 3 (different tumor subregions)\n",
        "# spatial_dims: 2 for 2D slices\n",
        "# channels: list of feature map numbers for each layer\n",
        "# strides: list of strides for each layer\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=4,  # T1, T1Gd, T2, FLAIR\n",
        "    out_channels=3, # Necrotic/Non-enhancing tumor core, Edema, Enhancing tumor\n",
        "    channels=(16, 32, 64, 128, 256), # Example feature map sizes\n",
        "    strides=(2, 2, 2, 2), # Example strides for downsampling\n",
        ").to(device)\n",
        "\n",
        "print(f\"UNET model created and moved to {device}.\")\n",
        "print(f\"Model input channels: {model.in_channels}\")\n",
        "print(f\"Model output channels: {model.out_channels}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c24ed3b"
      },
      "source": [
        "## Set Up Training Components\n",
        "\n",
        "### Subtask:\n",
        "Configure the essential components for model training: select an appropriate loss function (e.g., DiceLoss or a combination from MONAI), choose an optimizer (e.g., Adam), and set up the device (CPU or GPU) for computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4925e9d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The device has already been set up in the previous step. Now I need to define the loss function using `DiceLoss` from `monai.losses` and the optimizer using `torch.optim.Adam` with the model's parameters and a learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7709c93d"
      },
      "source": [
        "from monai.losses import DiceLoss\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device setup was already done in the previous step, ensuring model is on the correct device.\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Define the loss function\n",
        "# Using DiceLoss for segmentation. For multi-label where each channel is a binary prediction,\n",
        "# set sigmoid=True. to_onehot_y=False because our mask already has 3 channels for different tumor parts.\n",
        "loss_function = DiceLoss(sigmoid=True, to_onehot_y=False)\n",
        "\n",
        "# 3. Define the optimizer\n",
        "# Using Adam optimizer with a learning rate of 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), 1e-4)\n",
        "\n",
        "print(f\"Loss function defined: {loss_function.__class__.__name__}\")\n",
        "print(f\"Optimizer defined: {optimizer.__class__.__name__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ab65e8"
      },
      "source": [
        "## Train the UNET Model\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop for the UNET model, including forward and backward passes, loss calculation, weight updates, and periodic validation and checkpointing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cd233b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to set up the training loop as described, including defining epochs, initializing the DiceMetric, and then creating the main training loop with forward/backward passes, loss calculation, optimization, and a validation phase with metric calculation and checkpointing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c166b52"
      },
      "source": [
        "from monai.metrics import DiceMetric\n",
        "from monai.inferers import sliding_window_inference\n",
        "import torch\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# 1. Set the number of training epochs\n",
        "max_epochs = 50 # Reduced for demonstration, can be increased for full training\n",
        "\n",
        "# 2. Initialize DiceMetric for evaluation\n",
        "# include_background=False for ignoring background class in Dice calculation\n",
        "# For multi-class segmentation, typically each class is evaluated separately or averaged.\n",
        "# to_onehot_y is set to True if the ground truth mask is not one-hot encoded.\n",
        "# Our HDF5 mask is (H, W, 3) where the last dim might represent different classes, not one-hot\n",
        "# so we'll treat it as multi-label and apply sigmoid to prediction, DiceLoss assumes this.\n",
        "# However, DiceMetric expects one-hot for class-wise calculation unless specified otherwise.\n",
        "# Let's adjust for multi-label binary segmentation per channel.\n",
        "\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
        "\n",
        "# Keep track of best metric and best model weights\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "# Create a copy of the model's state_dict to save the best model weights\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        # print(f\"{step}/{len(train_loader)}, train_loss: {loss.item():.4f}\")\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    if (epoch + 1) % 5 == 0 or epoch == max_epochs -1: # Validate every 5 epochs or on the last epoch\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
        "                # Perform sliding window inference for larger images if necessary,\n",
        "                # but for 2D slices, direct inference is fine.\n",
        "                val_outputs = model(val_inputs)\n",
        "\n",
        "                # Apply sigmoid to outputs for DiceMetric as per DiceLoss setup\n",
        "                val_outputs = torch.sigmoid(val_outputs)\n",
        "\n",
        "                # DiceMetric expects one-hot encoding for ground truth if include_background is False and reduction is per_batch\n",
        "                # Given our labels are (B, C, H, W) where C=3, and each channel is a binary mask for a class,\n",
        "                # we should treat val_labels as already one-hot-like or multi-label binary.\n",
        "                # DiceMetric's default behavior for reduction=\"mean_batch\" with C>1 is to calculate for each class and average\n",
        "                # it expects predictions to be a probability map (after sigmoid) and labels to be one-hot. Here labels are 3-channel masks.\n",
        "                # For MONAI DiceMetric with multi-channel binary labels (0 or 1 per channel),\n",
        "                # if to_onehot_y is False, it will assume labels are already in the correct format (e.g. one-hot-like or directly comparable to logits).\n",
        "                # Since our mask is float32 (0 or 1), it matches. However, our mask is (B, 3, H, W), representing 3 independent binary segments\n",
        "                # which means we don't need to_onehot_y=True here. It will calculate Dice for each channel and average them.\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            metric_values.append(metric)\n",
        "            dice_metric.reset()\n",
        "\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_model_wts, \"best_metric_model.pth\")\n",
        "                print(f\"saved new best metric model at epoch {best_metric_epoch} with dice: {best_metric:.4f}\")\n",
        "            print(f\"Current epoch {epoch + 1} validation dice: {metric:.4f}\")\n",
        "            print(f\"Best validation dice: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "\n",
        "print(f\"Training completed. Best_metric: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}